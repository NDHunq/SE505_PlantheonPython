"""
Script ƒë·ªÉ ph√¢n t√≠ch v√† ch·ªçn ng∆∞·ª°ng tin c·∫≠y (confidence threshold) t·ªët nh·∫•t cho API
Ch·∫°y: python scripts/analyze_confidence_threshold.py
"""

import os
import json
import torch
import numpy as np
from PIL import Image
from torchvision import transforms
from timm import create_model
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from collections import defaultdict

# ===== C·∫§U H√åNH =====
DATASET_PATH = r"E:\Download\Dataset"  # THAY ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N N√ÄY
MODEL_PATH = r"BEST_MODEL.pth"
CLASS_NAMES_PATH = r"class_names.json"
OUTPUT_DIR = r"threshold_analysis"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ===== LOAD MODEL =====
print("ƒêang load model...")
with open(CLASS_NAMES_PATH, 'r', encoding='utf-8') as f:
    class_names = json.load(f)

num_classes = len(class_names)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = create_model('efficientnet_b4', pretrained=False, num_classes=num_classes)

# Load state dict v√† l·ªçc b·ªè c√°c key total_ops/total_params t·ª´ THOP profiling
state_dict = torch.load(MODEL_PATH, map_location=device, weights_only=False)
filtered_state = {k: v for k, v in state_dict.items() if "total_ops" not in k and "total_params" not in k}
model.load_state_dict(filtered_state, strict=False)

model.to(device)
model.eval()

# ===== TRANSFORM =====
transform = transforms.Compose([
    transforms.Resize(380),
    transforms.CenterCrop(380),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ===== ƒê·ªåC DATASET =====
print("ƒêang ƒë·ªçc dataset...")

# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n dataset
if not os.path.exists(DATASET_PATH):
    print(f"\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y dataset t·∫°i: {DATASET_PATH}")
    print(f"\nüìù H∆Ø·ªöNG D·∫™N:")
    print(f"   1. M·ªü file: scripts/analyze_confidence_threshold.py")
    print(f"   2. S·ª≠a d√≤ng 20: DATASET_PATH = r\"ƒê∆Ø·ªúNG_D·∫™N_DATASET_C·ª¶A_B·∫†N\"")
    print(f"   3. V√≠ d·ª•: DATASET_PATH = r\"E:\\Download\\Dataset\"")
    exit(1)

image_paths, labels = [], []

for class_name in sorted(os.listdir(DATASET_PATH)):
    class_dir = os.path.join(DATASET_PATH, class_name)
    if not os.path.isdir(class_dir): 
        continue
    for file in os.listdir(class_dir):
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            image_paths.append(os.path.join(class_dir, file))
            labels.append(class_name)

if len(image_paths) == 0:
    print(f"\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y ·∫£nh n√†o trong dataset!")
    print(f"   ƒê∆∞·ªùng d·∫´n: {DATASET_PATH}")
    print(f"   C·∫•u tr√∫c mong ƒë·ª£i:")
    print(f"   {DATASET_PATH}/")
    print(f"   ‚îú‚îÄ‚îÄ Tomato___Late_blight/")
    print(f"   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg")
    print(f"   ‚îÇ   ‚îî‚îÄ‚îÄ image2.jpg")
    print(f"   ‚îî‚îÄ‚îÄ Pepper___Bacterial_spot/")
    print(f"       ‚îî‚îÄ‚îÄ image3.jpg")
    exit(1)

print(f"‚úì T√¨m th·∫•y {len(image_paths)} ·∫£nh t·ª´ {len(set(labels))} l·ªõp")

class_to_idx = {c: i for i, c in enumerate(class_names)}
label_indices = [class_to_idx[l] for l in labels]

# Chia t·∫≠p test (10% cu·ªëi c√πng)
_, X_test, _, y_test = train_test_split(
    image_paths, label_indices, test_size=0.1, stratify=label_indices, random_state=42
)

print(f"S·ªë ·∫£nh test: {len(X_test)}")

# ===== D·ª∞ ƒêO√ÅN V√Ä L∆ØU X√ÅC SU·∫§T =====
print("\nƒêang d·ª± ƒëo√°n tr√™n t·∫≠p test...")
all_confidences = []  # X√°c su·∫•t cao nh·∫•t c·ªßa m·ªói ·∫£nh
correct_confidences = []  # X√°c su·∫•t khi d·ª± ƒëo√°n ƒë√∫ng
wrong_confidences = []  # X√°c su·∫•t khi d·ª± ƒëo√°n sai

with torch.no_grad():
    for img_path, true_label in tqdm(zip(X_test, y_test), total=len(X_test)):
        try:
            img = Image.open(img_path).convert('RGB')
            img_tensor = transform(img).unsqueeze(0).to(device)
            
            output = model(img_tensor)
            probs = torch.softmax(output, dim=1)
            max_prob, pred_label = probs.max(1)
            
            confidence = max_prob.item()
            all_confidences.append(confidence)
            
            if pred_label.item() == true_label:
                correct_confidences.append(confidence)
            else:
                wrong_confidences.append(confidence)
        except Exception as e:
            print(f"L·ªói x·ª≠ l√Ω {img_path}: {e}")
            continue

all_confidences = np.array(all_confidences)
correct_confidences = np.array(correct_confidences)
wrong_confidences = np.array(wrong_confidences)

# ===== PH√ÇN T√çCH NG∆Ø·ª†NG =====
print("\n" + "="*60)
print("PH√ÇN T√çCH CONFIDENCE THRESHOLD")
print("="*60)

# Th·ªëng k√™ c∆° b·∫£n
print(f"\nT·ªïng s·ªë ·∫£nh test: {len(all_confidences)}")
print(f"S·ªë ·∫£nh d·ª± ƒëo√°n ƒë√∫ng: {len(correct_confidences)} ({len(correct_confidences)/len(all_confidences)*100:.2f}%)")
print(f"S·ªë ·∫£nh d·ª± ƒëo√°n sai: {len(wrong_confidences)} ({len(wrong_confidences)/len(all_confidences)*100:.2f}%)")

print(f"\nConfidence trung b√¨nh:")
print(f"  - To√†n b·ªô: {all_confidences.mean():.4f}")
print(f"  - D·ª± ƒëo√°n ƒë√∫ng: {correct_confidences.mean():.4f}")
print(f"  - D·ª± ƒëo√°n sai: {wrong_confidences.mean():.4f}")

# Ph√¢n t√≠ch theo c√°c ng∆∞·ª°ng kh√°c nhau
print("\n" + "-"*60)
print("ƒê√ÅNH GI√Å C√ÅC NG∆Ø·ª†NG KH√ÅC NHAU")
print("-"*60)
print(f"{'Threshold':<12} {'Accept%':<10} {'Accuracy':<12} {'Reject%':<10}")
print("-"*60)

threshold_results = []
for threshold in np.arange(0.5, 1.0, 0.05):
    accepted_mask = all_confidences >= threshold
    num_accepted = accepted_mask.sum()
    
    if num_accepted > 0:
        accepted_correct = np.sum(np.array(correct_confidences) >= threshold)
        accuracy_at_threshold = accepted_correct / num_accepted
    else:
        accuracy_at_threshold = 0
    
    accept_rate = num_accepted / len(all_confidences)
    reject_rate = 1 - accept_rate
    
    threshold_results.append({
        'threshold': threshold,
        'accept_rate': accept_rate,
        'accuracy': accuracy_at_threshold,
        'reject_rate': reject_rate
    })
    
    print(f"{threshold:.2f}         {accept_rate*100:6.2f}%    {accuracy_at_threshold*100:6.2f}%      {reject_rate*100:6.2f}%")

# ===== VISUALIZATION =====
print("\nƒêang t·∫°o bi·ªÉu ƒë·ªì...")

# 1. Histogram c·ªßa confidence scores
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Subplot 1: Distribution c·ªßa correct vs wrong
axes[0, 0].hist(correct_confidences, bins=50, alpha=0.7, label='Correct', color='green', edgecolor='black')
axes[0, 0].hist(wrong_confidences, bins=50, alpha=0.7, label='Wrong', color='red', edgecolor='black')
axes[0, 0].set_xlabel('Confidence Score')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].set_title('Ph√¢n b·ªë Confidence: ƒê√∫ng vs Sai')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Subplot 2: Cumulative distribution
axes[0, 1].hist(all_confidences, bins=50, cumulative=True, alpha=0.7, color='blue', edgecolor='black')
axes[0, 1].set_xlabel('Confidence Score')
axes[0, 1].set_ylabel('Cumulative Count')
axes[0, 1].set_title('Ph√¢n b·ªë t√≠ch l≈©y Confidence')
axes[0, 1].grid(True, alpha=0.3)

# Subplot 3: Accept Rate vs Threshold
thresholds = [r['threshold'] for r in threshold_results]
accept_rates = [r['accept_rate'] * 100 for r in threshold_results]
axes[1, 0].plot(thresholds, accept_rates, marker='o', linewidth=2, markersize=6)
axes[1, 0].set_xlabel('Threshold')
axes[1, 0].set_ylabel('Accept Rate (%)')
axes[1, 0].set_title('T·ª∑ l·ªá Accept theo Threshold')
axes[1, 0].grid(True, alpha=0.3)

# Subplot 4: Accuracy at different thresholds
accuracies = [r['accuracy'] * 100 for r in threshold_results]
axes[1, 1].plot(thresholds, accuracies, marker='s', linewidth=2, markersize=6, color='green')
axes[1, 1].set_xlabel('Threshold')
axes[1, 1].set_ylabel('Accuracy (%)')
axes[1, 1].set_title('Accuracy khi √°p d·ª•ng Threshold')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'confidence_analysis.png'), dpi=300, bbox_inches='tight')
print(f"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {os.path.join(OUTPUT_DIR, 'confidence_analysis.png')}")

# ===== KHUY·∫æN NGH·ªä =====
print("\n" + "="*60)
print("KHUY·∫æN NGH·ªä NG∆Ø·ª†NG TIN C·∫¨Y")
print("="*60)

# T√¨m ng∆∞·ª°ng t·ªëi ∆∞u (balance gi·ªØa accuracy v√† accept rate)
# Ng∆∞·ª°ng t·ªët: accuracy > 95% v√† accept rate > 80%
optimal_thresholds = [
    r for r in threshold_results 
    if r['accuracy'] >= 0.95 and r['accept_rate'] >= 0.80
]

if optimal_thresholds:
    best = optimal_thresholds[0]
    print(f"\n‚úÖ NG∆Ø·ª†NG ƒê·ªÄ XU·∫§T: {best['threshold']:.2f}")
    print(f"   - Accept Rate: {best['accept_rate']*100:.2f}%")
    print(f"   - Accuracy: {best['accuracy']*100:.2f}%")
    print(f"   - Reject Rate: {best['reject_rate']*100:.2f}%")
else:
    # T√¨m threshold c√≥ accuracy cao nh·∫•t m√† v·∫´n accept > 70%
    viable = [r for r in threshold_results if r['accept_rate'] >= 0.70]
    if viable:
        best = max(viable, key=lambda x: x['accuracy'])
        print(f"\n‚ö†Ô∏è  NG∆Ø·ª†NG ƒê·ªÄ XU·∫§T (relaxed): {best['threshold']:.2f}")
        print(f"   - Accept Rate: {best['accept_rate']*100:.2f}%")
        print(f"   - Accuracy: {best['accuracy']*100:.2f}%")
        print(f"   - Reject Rate: {best['reject_rate']*100:.2f}%")
    else:
        print("\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y ng∆∞·ª°ng ph√π h·ª£p. Xem x√©t l·∫°i m√¥ h√¨nh.")

# L∆∞u k·∫øt qu·∫£
results_file = os.path.join(OUTPUT_DIR, 'threshold_analysis.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump({
        'statistics': {
            'total_samples': len(all_confidences),
            'correct_predictions': len(correct_confidences),
            'wrong_predictions': len(wrong_confidences),
            'avg_confidence_all': float(all_confidences.mean()),
            'avg_confidence_correct': float(correct_confidences.mean()),
            'avg_confidence_wrong': float(wrong_confidences.mean())
        },
        'threshold_analysis': threshold_results
    }, f, indent=2)

print(f"\n‚úÖ K·∫øt qu·∫£ chi ti·∫øt ƒë√£ l∆∞u t·∫°i: {results_file}")
print("\n" + "="*60)
